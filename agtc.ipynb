{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232edaec"
      },
      "source": [
        "## Clone the repository\n",
        "\n",
        "### Subtask:\n",
        "Clone the AGTC repository if not already done.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd055b32"
      },
      "source": [
        "**Reasoning**:\n",
        "Clone the AGTC repository using git clone.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbe0f900",
        "outputId": "6bbdf8a5-f112-415b-be76-d76ddbcb6d6e"
      },
      "source": [
        "!git clone https://github.com/mtntruong/AGTC.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AGTC'...\n",
            "remote: Enumerating objects: 421, done.\u001b[K\n",
            "remote: Counting objects: 100% (311/311), done.\u001b[K\n",
            "remote: Compressing objects: 100% (235/235), done.\u001b[K\n",
            "remote: Total 421 (delta 84), reused 293 (delta 74), pack-reused 110 (from 1)\u001b[K\n",
            "Receiving objects: 100% (421/421), 173.97 MiB | 36.25 MiB/s, done.\n",
            "Resolving deltas: 100% (126/126), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "245a3517"
      },
      "source": [
        "## Identify .mat files\n",
        "\n",
        "### Subtask:\n",
        "Find all `.mat` files within the cloned AGTC repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a44359c"
      },
      "source": [
        "**Reasoning**:\n",
        "Walk through the cloned repository directory to find all .mat files and store their paths in a list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2144119"
      },
      "source": [
        "# Task\n",
        "Run the AGTC-HSI model from the cloned repository \"https://github.com/mtntruong/AGTC.git\", convert all necessary .mat files within the AGTC-HSI component to .npy format, train the model, and provide code to download the output files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d26c224"
      },
      "source": [
        "## Identify agtc-hsi related .mat files\n",
        "\n",
        "### Subtask:\n",
        "Find the `.mat` files specifically used by the AGTC-HSI component within the cloned repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a880a3e"
      },
      "source": [
        "**Reasoning**:\n",
        "Walk through the cloned repository and identify the `.mat` files relevant to the HSI component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2cec799",
        "outputId": "3f0bc6dd-9cf0-436e-9945-86dc06b0b304"
      },
      "source": [
        "import os\n",
        "\n",
        "hsi_mat_files = []\n",
        "for root, dirs, files in os.walk('AGTC'):\n",
        "    for file in files:\n",
        "        if file.endswith('.mat'):\n",
        "            if 'hsi' in root.lower() or 'agtc-hsi' in root.lower():\n",
        "                hsi_mat_files.append(os.path.join(root, file))\n",
        "\n",
        "print(hsi_mat_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AGTC/AGTC-HSI/PaviaU/PaviaU_test.mat', 'AGTC/AGTC-HSI/PaviaU/Data-Preparation/PaviaU.mat', 'AGTC/AGTC-HSI/PaviaU/Metrics/PaviaU_ground_truth.mat', 'AGTC/AGTC-HSI/Landsat/Landsat_test.mat', 'AGTC/AGTC-HSI/Landsat/Data-Preparation/LE71520182003140ASN00_3001_3512.mat', 'AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_mask.mat', 'AGTC/AGTC-HSI/Landsat/Data-Preparation/LE07_L1TP_109018_20210727_20210727_01_RT_GM_3001_3512.mat', 'AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_clean.mat', 'AGTC/AGTC-HSI/Landsat/Metrics/Landsat_ground_truth.mat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fdd2743"
      },
      "source": [
        "## Convert agtc-hsi .mat files to .npy\n",
        "\n",
        "### Subtask:\n",
        "Load each identified AGTC-HSI `.mat` file and save it as a `.npy` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "289ee729"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the identified AGTC-HSI .mat files, load each one, extract the relevant data, and save it as a .npy file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43d0f01e",
        "outputId": "8a4648cc-8d8b-41e4-8441-b815dd7a56fd"
      },
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "\n",
        "for mat_file_path in hsi_mat_files:\n",
        "    try:\n",
        "        mat_data = sio.loadmat(mat_file_path)\n",
        "\n",
        "        # Determine the relevant key by inspecting the loaded data\n",
        "        # This is a heuristic and might need adjustment based on the specific .mat files\n",
        "        relevant_key = None\n",
        "        for key in mat_data:\n",
        "            if not key.startswith('__'): # Ignore internal keys\n",
        "                if isinstance(mat_data[key], np.ndarray):\n",
        "                    relevant_key = key\n",
        "                    break # Assume the first array is the relevant one\n",
        "\n",
        "        if relevant_key is not None:\n",
        "            array_data = mat_data[relevant_key]\n",
        "\n",
        "            npy_file_path = mat_file_path.replace('.mat', '.npy')\n",
        "            np.save(npy_file_path, array_data)\n",
        "            print(f\"Converted '{mat_file_path}' to '{npy_file_path}' using key '{relevant_key}'\")\n",
        "        else:\n",
        "            print(f\"Could not find a suitable array in '{mat_file_path}' to convert.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting '{mat_file_path}': {e}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error converting 'AGTC/AGTC-HSI/PaviaU/PaviaU_test.mat': Please use HDF reader for matlab v7.3 files, e.g. h5py\n",
            "Converted 'AGTC/AGTC-HSI/PaviaU/Data-Preparation/PaviaU.mat' to 'AGTC/AGTC-HSI/PaviaU/Data-Preparation/PaviaU.npy' using key 'paviaU'\n",
            "Converted 'AGTC/AGTC-HSI/PaviaU/Metrics/PaviaU_ground_truth.mat' to 'AGTC/AGTC-HSI/PaviaU/Metrics/PaviaU_ground_truth.npy' using key 'paviaU'\n",
            "Error converting 'AGTC/AGTC-HSI/Landsat/Landsat_test.mat': Please use HDF reader for matlab v7.3 files, e.g. h5py\n",
            "Converted 'AGTC/AGTC-HSI/Landsat/Data-Preparation/LE71520182003140ASN00_3001_3512.mat' to 'AGTC/AGTC-HSI/Landsat/Data-Preparation/LE71520182003140ASN00_3001_3512.npy' using key 'LE71520182003140ASN00_3001_3512'\n",
            "Converted 'AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_mask.mat' to 'AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_mask.npy' using key 'Landsat7_training_mask'\n",
            "Converted 'AGTC/AGTC-HSI/Landsat/Data-Preparation/LE07_L1TP_109018_20210727_20210727_01_RT_GM_3001_3512.mat' to 'AGTC/AGTC-HSI/Landsat/Data-Preparation/LE07_L1TP_109018_20210727_20210727_01_RT_GM_3001_3512.npy' using key 'LE07_L1TP_109018_20210727_20210727_01_RT_GM_3001_3512'\n",
            "Converted 'AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_clean.mat' to 'AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_clean.npy' using key 'Landsat7_training_clean'\n",
            "Converted 'AGTC/AGTC-HSI/Landsat/Metrics/Landsat_ground_truth.mat' to 'AGTC/AGTC-HSI/Landsat/Metrics/Landsat_ground_truth.npy' using key 'Landsat_ground_truth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffa7e2aa"
      },
      "source": [
        "**Reasoning**:\n",
        "Some of the .mat files are in a newer format (v7.3) that requires a different reader. Use the `h5py` library to load these files and save them as .npy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e08041cb",
        "outputId": "aaa0ed9b-4c69-4294-b9fc-4575ceecc2a7"
      },
      "source": [
        "import h5py\n",
        "\n",
        "for mat_file_path in hsi_mat_files:\n",
        "    if '.mat' in mat_file_path and 'test' in mat_file_path: # Target the files that failed previously\n",
        "        try:\n",
        "            with h5py.File(mat_file_path, 'r') as f:\n",
        "                # Inspect the keys within the HDF5 file to find the data\n",
        "                # This might require manual inspection if the key names are not consistent\n",
        "                # For this case, based on typical HSI data structures, we'll assume a key.\n",
        "                # If this fails, manual inspection would be necessary.\n",
        "                # Let's try a few common key names or assume the first dataset key is the data\n",
        "                relevant_key = None\n",
        "                for key in f.keys():\n",
        "                    if isinstance(f[key], h5py.Dataset):\n",
        "                         relevant_key = key\n",
        "                         break # Assume the first dataset key is the relevant one\n",
        "\n",
        "                if relevant_key is not None:\n",
        "                    array_data = f[relevant_key][()] # Load the data\n",
        "                    npy_file_path = mat_file_path.replace('.mat', '.npy')\n",
        "                    np.save(npy_file_path, array_data)\n",
        "                    print(f\"Converted '{mat_file_path}' to '{npy_file_path}' using key '{relevant_key}'\")\n",
        "                else:\n",
        "                    print(f\"Could not find a suitable dataset in '{mat_file_path}' to convert.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting '{mat_file_path}' using h5py: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 'AGTC/AGTC-HSI/PaviaU/PaviaU_test.mat' to 'AGTC/AGTC-HSI/PaviaU/PaviaU_test.npy' using key 'Nmsi'\n",
            "Converted 'AGTC/AGTC-HSI/Landsat/Landsat_test.mat' to 'AGTC/AGTC-HSI/Landsat/Landsat_test.npy' using key 'Nmsi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a8d32f1"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install any necessary libraries or dependencies required to run the AGTC-HSI code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ff29b3"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for a requirements.txt file in the cloned repository and install dependencies if found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72d19b09",
        "outputId": "ad6340d3-d62b-4c9c-fae0-c0fc8043d6d3"
      },
      "source": [
        "import os\n",
        "\n",
        "repo_dir = 'AGTC'\n",
        "requirements_path = os.path.join(repo_dir, 'requirements.txt')\n",
        "\n",
        "if os.path.exists(requirements_path):\n",
        "    print(f\"Found requirements.txt at {requirements_path}. Installing dependencies...\")\n",
        "    !pip install -r {requirements_path}\n",
        "else:\n",
        "    print(\"No requirements.txt found. Will need to manually identify and install dependencies.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No requirements.txt found. Will need to manually identify and install dependencies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c700e98"
      },
      "source": [
        "**Reasoning**:\n",
        "No requirements.txt was found, so examine the AGTC-HSI code to identify necessary libraries and install common data science and deep learning libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b98269a",
        "outputId": "13fc7876-c041-4749-8ecd-d74fd8077856"
      },
      "source": [
        "# Based on the nature of the project (HSI processing, deep learning),\n",
        "# common libraries will likely include torch, numpy, scipy, matplotlib.\n",
        "# We will install these common libraries.\n",
        "\n",
        "print(\"Installing common data science and deep learning libraries...\")\n",
        "!pip install torch torchvision torchaudio numpy scipy matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing common data science and deep learning libraries...\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0351b172"
      },
      "source": [
        "**Reasoning**:\n",
        "The common data science and deep learning libraries are already satisfied. Examine the AGTC-HSI scripts more closely to identify any other specific libraries or modules that might be needed, and install them if necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc2cbf20",
        "outputId": "e9b792fd-dea5-4fa5-cebc-da41709c1449"
      },
      "source": [
        "# Based on a quick inspection of the AGTC-HSI directory and typical dependencies\n",
        "# for this type of model, we might need 'tqdm' for progress bars and potentially\n",
        "# other specific libraries depending on the implementation details.\n",
        "# We will install tqdm as it's a common utility. If other errors arise during\n",
        "# subsequent steps due to missing libraries, we will install them then.\n",
        "\n",
        "print(\"Installing additional potential dependencies (e.g., tqdm)...\")\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing additional potential dependencies (e.g., tqdm)...\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76bcdb13"
      },
      "source": [
        "## Prepare data for agtc-hsi\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the data (including the converted `.npy` files) specifically for the AGTC-HSI model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd791a16"
      },
      "source": [
        "**Reasoning**:\n",
        "Navigate to the AGTC-HSI directory, inspect the files, and load and preprocess the necessary .npy data for the AGTC-HSI model based on common practices and potential script analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "844652bc",
        "outputId": "bbd59845-3cf3-4cbe-8260-c697e883afa0"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Navigate to the AGTC-HSI directory\n",
        "hsi_dir = os.path.join('AGTC', 'AGTC-HSI')\n",
        "print(f\"Navigating to {hsi_dir}\")\n",
        "\n",
        "# Inspect files and directories (simulated inspection)\n",
        "# In a real scenario, we would manually look at files like main.py, train.py, etc.\n",
        "# to understand data loading. Based on typical HSI examples and the converted files,\n",
        "# we expect data and ground truth files (e.g., PaviaU.npy, PaviaU_gt.npy).\n",
        "\n",
        "# Load the necessary .npy files. Assuming PaviaU dataset for demonstration.\n",
        "# We need to find the actual file paths based on the previous conversion step.\n",
        "\n",
        "pavia_u_data_path = None\n",
        "pavia_u_gt_path = None\n",
        "\n",
        "for root, dirs, files in os.walk(hsi_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.npy'):\n",
        "            if 'paviau' in file.lower():\n",
        "                if '_gt' in file.lower():\n",
        "                    pavia_u_gt_path = os.path.join(root, file)\n",
        "                else:\n",
        "                    pavia_u_data_path = os.path.join(root, file)\n",
        "\n",
        "if pavia_u_data_path and pavia_u_gt_path:\n",
        "    print(f\"Loading data from: {pavia_u_data_path}\")\n",
        "    print(f\"Loading ground truth from: {pavia_u_gt_path}\")\n",
        "\n",
        "    # Load the .npy files\n",
        "    pavia_u_data = np.load(pavia_u_data_path)\n",
        "    pavia_u_gt = np.load(pavia_u_gt_path)\n",
        "\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(\"PaviaU Data shape:\", pavia_u_data.shape)\n",
        "    print(\"PaviaU Ground Truth shape:\", pavia_u_gt.shape)\n",
        "\n",
        "    # Perform basic preprocessing (e.g., normalization) if needed.\n",
        "    # Based on typical HSI data, values are often in a specific range.\n",
        "    # Normalizing to [0, 1] is a common step.\n",
        "    # Check data range before normalization\n",
        "    data_min = np.min(pavia_u_data)\n",
        "    data_max = np.max(pavia_u_data)\n",
        "    print(f\"PaviaU Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "    # Avoid division by zero if data_max is 0\n",
        "    if data_max > 0:\n",
        "        pavia_u_data_normalized = pavia_u_data / data_max\n",
        "        print(\"PaviaU Data normalized to [0, 1].\")\n",
        "        # Store the prepared data\n",
        "        prepared_hsi_data = {\n",
        "            'data': pavia_u_data_normalized,\n",
        "            'gt': pavia_u_gt\n",
        "        }\n",
        "        print(\"Prepared data stored in 'prepared_hsi_data'.\")\n",
        "    else:\n",
        "        print(\"Data max is 0, skipping normalization.\")\n",
        "        # Store the prepared data without normalization\n",
        "        prepared_hsi_data = {\n",
        "            'data': pavia_u_data,\n",
        "            'gt': pavia_u_gt\n",
        "        }\n",
        "        print(\"Prepared data stored in 'prepared_hsi_data' without normalization.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Could not find both PaviaU data and ground truth .npy files.\")\n",
        "    prepared_hsi_data = None # Indicate failure to load data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to AGTC/AGTC-HSI\n",
            "Could not find both PaviaU data and ground truth .npy files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a094a2a9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to find the PaviaU .npy files within the 'AGTC/AGTC-HSI' directory failed. I need to broaden the search to the entire 'AGTC' directory, as the previous conversion step saved the .npy files in the same location as the original .mat files, which might not be within the AGTC-HSI subdirectory. I will also add code to inspect the found files and their keys to understand their structure before loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84d5b4f3",
        "outputId": "bcadaf89-2aa9-462a-aab1-4e7eefa25a84"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Search for .npy files within the entire AGTC directory\n",
        "agtc_root_dir = 'AGTC'\n",
        "pavia_u_data_path = None\n",
        "pavia_u_gt_path = None\n",
        "landsat_data_path = None\n",
        "landsat_gt_path = None\n",
        "\n",
        "\n",
        "print(f\"Searching for .npy files within '{agtc_root_dir}'...\")\n",
        "\n",
        "for root, dirs, files in os.walk(agtc_root_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.npy'):\n",
        "            lower_file = file.lower()\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            if 'paviau' in lower_file:\n",
        "                if '_gt' in lower_file:\n",
        "                    pavia_u_gt_path = file_path\n",
        "                else:\n",
        "                    pavia_u_data_path = file_path\n",
        "            elif 'landsat' in lower_file:\n",
        "                 if '_gt' in lower_file:\n",
        "                    landsat_gt_path = file_path\n",
        "                 else:\n",
        "                    landsat_data_path = file_path\n",
        "\n",
        "print(\"Search complete.\")\n",
        "\n",
        "# Attempt to load PaviaU data first, as it's a common HSI dataset\n",
        "if pavia_u_data_path and pavia_u_gt_path:\n",
        "    print(f\"Found PaviaU data at: {pavia_u_data_path}\")\n",
        "    print(f\"Found PaviaU ground truth at: {pavia_u_gt_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load the .npy files\n",
        "        pavia_u_data = np.load(pavia_u_data_path)\n",
        "        pavia_u_gt = np.load(pavia_u_gt_path)\n",
        "\n",
        "        print(\"PaviaU data loaded successfully.\")\n",
        "        print(\"PaviaU Data shape:\", pavia_u_data.shape)\n",
        "        print(\"PaviaU Ground Truth shape:\", pavia_u_gt.shape)\n",
        "\n",
        "        # Perform basic preprocessing (e.g., normalization) if needed.\n",
        "        # Check data range before normalization\n",
        "        data_min = np.min(pavia_u_data)\n",
        "        data_max = np.max(pavia_u_data)\n",
        "        print(f\"PaviaU Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "        # Avoid division by zero if data_max is 0\n",
        "        if data_max > 0:\n",
        "            pavia_u_data_normalized = pavia_u_data / data_max\n",
        "            print(\"PaviaU Data normalized to [0, 1].\")\n",
        "            # Store the prepared data\n",
        "            prepared_hsi_data = {\n",
        "                'data': pavia_u_data_normalized,\n",
        "                'gt': pavia_u_gt,\n",
        "                'dataset': 'PaviaU'\n",
        "            }\n",
        "            print(\"Prepared data for PaviaU stored in 'prepared_hsi_data'.\")\n",
        "        else:\n",
        "            print(\"PaviaU Data max is 0, skipping normalization.\")\n",
        "            # Store the prepared data without normalization\n",
        "            prepared_hsi_data = {\n",
        "                'data': pavia_u_data,\n",
        "                'gt': pavia_u_gt,\n",
        "                'dataset': 'PaviaU'\n",
        "            }\n",
        "            print(\"Prepared data for PaviaU stored in 'prepared_hsi_data' without normalization.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing PaviaU data: {e}\")\n",
        "        prepared_hsi_data = None\n",
        "\n",
        "elif landsat_data_path and landsat_gt_path:\n",
        "    print(f\"Found Landsat data at: {landsat_data_path}\")\n",
        "    print(f\"Found Landsat ground truth at: {landsat_gt_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load the .npy files\n",
        "        landsat_data = np.load(landsat_data_path)\n",
        "        landsat_gt = np.load(landsat_gt_path)\n",
        "\n",
        "        print(\"Landsat data loaded successfully.\")\n",
        "        print(\"Landsat Data shape:\", landsat_data.shape)\n",
        "        print(\"Landsat Ground Truth shape:\", landsat_gt.shape)\n",
        "\n",
        "        # Perform basic preprocessing (e.g., normalization) if needed.\n",
        "        # Check data range before normalization\n",
        "        data_min = np.min(landsat_data)\n",
        "        data_max = np.max(landsat_data)\n",
        "        print(f\"Landsat Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "        # Avoid division by zero if data_max is 0\n",
        "        if data_max > 0:\n",
        "            landsat_data_normalized = landsat_data / data_max\n",
        "            print(\"Landsat Data normalized to [0, 1].\")\n",
        "            # Store the prepared data\n",
        "            prepared_hsi_data = {\n",
        "                'data': landsat_data_normalized,\n",
        "                'gt': landsat_gt,\n",
        "                'dataset': 'Landsat'\n",
        "            }\n",
        "            print(\"Prepared data for Landsat stored in 'prepared_hsi_data'.\")\n",
        "        else:\n",
        "            print(\"Landsat Data max is 0, skipping normalization.\")\n",
        "            # Store the prepared data without normalization\n",
        "            prepared_hsi_data = {\n",
        "                'data': landsat_data,\n",
        "                'gt': landsat_gt,\n",
        "                'dataset': 'Landsat'\n",
        "            }\n",
        "            print(\"Prepared data for Landsat stored in 'prepared_hsi_data' without normalization.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing Landsat data: {e}\")\n",
        "        prepared_hsi_data = None\n",
        "\n",
        "else:\n",
        "    print(\"Could not find complete .npy files for either PaviaU or Landsat datasets.\")\n",
        "    prepared_hsi_data = None # Indicate failure to load data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for .npy files within 'AGTC'...\n",
            "Search complete.\n",
            "Could not find complete .npy files for either PaviaU or Landsat datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5c17dd4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous search for `.npy` files failed because the file names might not exactly match 'paviau' or 'landsat', or the ground truth files might be named differently. I need to list all found `.npy` files within the `AGTC` directory and their shapes to understand what was actually converted and how they are named, then adjust the loading logic based on the actual file names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc519c3a",
        "outputId": "cc42a806-7879-4243-9f45-4b3191276c2f"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# List all .npy files found in the AGTC directory\n",
        "agtc_root_dir = 'AGTC'\n",
        "npy_files_found = []\n",
        "\n",
        "print(f\"Listing all .npy files found within '{agtc_root_dir}'...\")\n",
        "\n",
        "for root, dirs, files in os.walk(agtc_root_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.npy'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            npy_files_found.append(file_path)\n",
        "\n",
        "if npy_files_found:\n",
        "    print(\"Found .npy files:\")\n",
        "    for npy_file_path in npy_files_found:\n",
        "        try:\n",
        "            # Load a small part or get shape without loading everything\n",
        "            # This is a bit tricky with .npy, full load might be necessary to get shape reliably\n",
        "            # Let's try loading and printing shape, assuming files are not excessively large\n",
        "            data = np.load(npy_file_path)\n",
        "            print(f\"- {npy_file_path} (Shape: {data.shape})\")\n",
        "            del data # Release memory\n",
        "        except Exception as e:\n",
        "            print(f\"- {npy_file_path} (Error loading to get shape: {e})\")\n",
        "\n",
        "    # Now, based on the printed list and shapes, try to identify and load the correct files.\n",
        "    # Looking at the previous successful conversion output, the files should be named\n",
        "    # like 'AGTC/AGTC-HSI/Dataset/PaviaU.npy', 'AGTC/AGTC-HSI/Dataset/PaviaU_gt.npy', etc.\n",
        "\n",
        "    pavia_u_data_path = None\n",
        "    pavia_u_gt_path = None\n",
        "    landsat_data_path = None\n",
        "    landsat_gt_path = None\n",
        "\n",
        "    for file_path in npy_files_found:\n",
        "        lower_file_path = file_path.lower()\n",
        "        if 'paviau' in lower_file_path:\n",
        "            if '_gt' in lower_file_path:\n",
        "                pavia_u_gt_path = file_path\n",
        "            else:\n",
        "                pavia_u_data_path = file_path\n",
        "        elif 'landsat' in lower_file_path:\n",
        "             if '_gt' in lower_file_path:\n",
        "                landsat_gt_path = file_path\n",
        "             else:\n",
        "                landsat_data_path = file_path\n",
        "\n",
        "    # Now attempt to load and preprocess using the identified paths\n",
        "    if pavia_u_data_path and pavia_u_gt_path:\n",
        "        print(f\"Identified PaviaU data at: {pavia_u_data_path}\")\n",
        "        print(f\"Identified PaviaU ground truth at: {pavia_u_gt_path}\")\n",
        "\n",
        "        try:\n",
        "            pavia_u_data = np.load(pavia_u_data_path)\n",
        "            pavia_u_gt = np.load(pavia_u_gt_path)\n",
        "\n",
        "            print(\"PaviaU data loaded successfully.\")\n",
        "            print(\"PaviaU Data shape:\", pavia_u_data.shape)\n",
        "            print(\"PaviaU Ground Truth shape:\", pavia_u_gt.shape)\n",
        "\n",
        "            data_min = np.min(pavia_u_data)\n",
        "            data_max = np.max(pavia_u_data)\n",
        "            print(f\"PaviaU Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "            if data_max > 0:\n",
        "                pavia_u_data_normalized = pavia_u_data / data_max\n",
        "                print(\"PaviaU Data normalized to [0, 1].\")\n",
        "                prepared_hsi_data = {\n",
        "                    'data': pavia_u_data_normalized,\n",
        "                    'gt': pavia_u_gt,\n",
        "                    'dataset': 'PaviaU'\n",
        "                }\n",
        "                print(\"Prepared data for PaviaU stored in 'prepared_hsi_data'.\")\n",
        "            else:\n",
        "                print(\"PaviaU Data max is 0, skipping normalization.\")\n",
        "                prepared_hsi_data = {\n",
        "                    'data': pavia_u_data,\n",
        "                    'gt': pavia_u_gt,\n",
        "                    'dataset': 'PaviaU'\n",
        "                }\n",
        "                print(\"Prepared data for PaviaU stored in 'prepared_hsi_data' without normalization.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing PaviaU data: {e}\")\n",
        "            prepared_hsi_data = None\n",
        "\n",
        "    elif landsat_data_path and landsat_gt_path:\n",
        "        print(f\"Identified Landsat data at: {landsat_data_path}\")\n",
        "        print(f\"Identified Landsat ground truth at: {landsat_gt_path}\")\n",
        "\n",
        "        try:\n",
        "            landsat_data = np.load(landsat_data_path)\n",
        "            landsat_gt = np.load(landsat_gt_path)\n",
        "\n",
        "            print(\"Landsat data loaded successfully.\")\n",
        "            print(\"Landsat Data shape:\", landsat_data.shape)\n",
        "            print(\"Landsat Ground Truth shape:\", landsat_gt.shape)\n",
        "\n",
        "            data_min = np.min(landsat_data)\n",
        "            data_max = np.max(landsat_data)\n",
        "            print(f\"Landsat Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "            if data_max > 0:\n",
        "                landsat_data_normalized = landsat_data / data_max\n",
        "                print(\"Landsat Data normalized to [0, 1].\")\n",
        "                prepared_hsi_data = {\n",
        "                    'data': landsat_data_normalized,\n",
        "                    'gt': landsat_gt,\n",
        "                    'dataset': 'Landsat'\n",
        "                }\n",
        "                print(\"Prepared data for Landsat stored in 'prepared_hsi_data'.\")\n",
        "            else:\n",
        "                print(\"Landsat Data max is 0, skipping normalization.\")\n",
        "                prepared_hsi_data = {\n",
        "                    'data': landsat_data,\n",
        "                    'gt': landsat_gt,\n",
        "                    'dataset': 'Landsat'\n",
        "                }\n",
        "                print(\"Prepared data for Landsat stored in 'prepared_hsi_data' without normalization.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing Landsat data: {e}\")\n",
        "            prepared_hsi_data = None\n",
        "\n",
        "    else:\n",
        "        print(\"Could not identify complete .npy files for either PaviaU or Landsat datasets based on names.\")\n",
        "        prepared_hsi_data = None\n",
        "\n",
        "else:\n",
        "    print(\"No .npy files found in the AGTC directory.\")\n",
        "    prepared_hsi_data = None\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing all .npy files found within 'AGTC'...\n",
            "Found .npy files:\n",
            "- AGTC/AGTC-HSI/PaviaU/PaviaU_test.npy (Shape: (103, 256, 256))\n",
            "- AGTC/AGTC-HSI/PaviaU/Data-Preparation/PaviaU.npy (Shape: (610, 340, 103))\n",
            "- AGTC/AGTC-HSI/PaviaU/Metrics/PaviaU_ground_truth.npy (Shape: (610, 340, 103))\n",
            "- AGTC/AGTC-HSI/Landsat/Landsat_test.npy (Shape: (8, 512, 512))\n",
            "- AGTC/AGTC-HSI/Landsat/Data-Preparation/LE71520182003140ASN00_3001_3512.npy (Shape: (512, 512, 8))\n",
            "- AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_clean.npy (Shape: (1501, 1401, 8))\n",
            "- AGTC/AGTC-HSI/Landsat/Data-Preparation/Landsat7_training_mask.npy (Shape: (1501, 1401, 8))\n",
            "- AGTC/AGTC-HSI/Landsat/Data-Preparation/LE07_L1TP_109018_20210727_20210727_01_RT_GM_3001_3512.npy (Shape: (512, 512, 8))\n",
            "- AGTC/AGTC-HSI/Landsat/Metrics/Landsat_ground_truth.npy (Shape: (512, 512, 8))\n",
            "Could not identify complete .npy files for either PaviaU or Landsat datasets based on names.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "821138c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The listing of .npy files shows that the ground truth files are named `PaviaU_ground_truth.npy` and `Landsat_ground_truth.npy`, not `_gt.npy` as I assumed. Also, the data files have different names like `PaviaU.npy` and `LE71520182003140ASN00_3001_3512.npy`, `Landsat7_training_clean.npy`, etc. I need to update the logic to correctly identify the data and ground truth files based on these observed names and shapes, prioritizing the main data files like `PaviaU.npy` and the clean Landsat training data (`Landsat7_training_clean.npy`). I will then proceed with loading and preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ee787e6",
        "outputId": "13dbf6b7-f7f7-43c0-8944-b6060a7c43ba"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# List all .npy files found in the AGTC directory (already done, reusing the list)\n",
        "agtc_root_dir = 'AGTC'\n",
        "npy_files_found = []\n",
        "\n",
        "for root, dirs, files in os.walk(agtc_root_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.npy'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            npy_files_found.append(file_path)\n",
        "\n",
        "# Identify the correct data and ground truth files based on observed names and shapes\n",
        "pavia_u_data_path = None\n",
        "pavia_u_gt_path = None\n",
        "landsat_data_path = None # Prioritizing clean training data\n",
        "landsat_gt_path = None\n",
        "\n",
        "for file_path in npy_files_found:\n",
        "    lower_file_path = file_path.lower()\n",
        "\n",
        "    if 'paviau' in lower_file_path:\n",
        "        if 'ground_truth' in lower_file_path:\n",
        "            pavia_u_gt_path = file_path\n",
        "        elif 'paviau.npy' in lower_file_path: # Look for the main data file\n",
        "            pavia_u_data_path = file_path\n",
        "    elif 'landsat' in lower_file_path:\n",
        "         if 'ground_truth' in lower_file_path:\n",
        "            landsat_gt_path = file_path\n",
        "         elif 'landsat7_training_clean.npy' in lower_file_path: # Look for the clean training data\n",
        "            landsat_data_path = file_path\n",
        "\n",
        "\n",
        "# Now attempt to load and preprocess using the identified paths\n",
        "prepared_hsi_data = None # Initialize to None\n",
        "\n",
        "if pavia_u_data_path and pavia_u_gt_path:\n",
        "    print(f\"Identified PaviaU data at: {pavia_u_data_path}\")\n",
        "    print(f\"Identified PaviaU ground truth at: {pavia_u_gt_path}\")\n",
        "\n",
        "    try:\n",
        "        pavia_u_data = np.load(pavia_u_data_path)\n",
        "        pavia_u_gt = np.load(pavia_u_gt_path)\n",
        "\n",
        "        print(\"PaviaU data loaded successfully.\")\n",
        "        print(\"PaviaU Data shape:\", pavia_u_data.shape)\n",
        "        print(\"PaviaU Ground Truth shape:\", pavia_u_gt.shape)\n",
        "\n",
        "        # Basic preprocessing: Normalization\n",
        "        data_min = np.min(pavia_u_data)\n",
        "        data_max = np.max(pavia_u_data)\n",
        "        print(f\"PaviaU Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "        if data_max > 0:\n",
        "            pavia_u_data_normalized = pavia_u_data / data_max\n",
        "            print(\"PaviaU Data normalized to [0, 1].\")\n",
        "            prepared_hsi_data = {\n",
        "                'data': pavia_u_data_normalized,\n",
        "                'gt': pavia_u_gt,\n",
        "                'dataset': 'PaviaU'\n",
        "            }\n",
        "            print(\"Prepared data for PaviaU stored in 'prepared_hsi_data'.\")\n",
        "        else:\n",
        "            print(\"PaviaU Data max is 0, skipping normalization.\")\n",
        "            prepared_hsi_data = {\n",
        "                'data': pavia_u_data,\n",
        "                'gt': pavia_u_gt,\n",
        "                'dataset': 'PaviaU'\n",
        "            }\n",
        "            print(\"Prepared data for PaviaU stored in 'prepared_hsi_data' without normalization.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing PaviaU data: {e}\")\n",
        "        prepared_hsi_data = None\n",
        "\n",
        "elif landsat_data_path and landsat_gt_path:\n",
        "    print(f\"Identified Landsat data at: {landsat_data_path}\")\n",
        "    print(f\"Identified Landsat ground truth at: {landsat_gt_path}\")\n",
        "\n",
        "    try:\n",
        "        landsat_data = np.load(landsat_data_path)\n",
        "        landsat_gt = np.load(landsat_gt_path)\n",
        "\n",
        "        print(\"Landsat data loaded successfully.\")\n",
        "        print(\"Landsat Data shape:\", landsat_data.shape)\n",
        "        print(\"Landsat Ground Truth shape:\", landsat_gt.shape)\n",
        "\n",
        "        # Basic preprocessing: Normalization\n",
        "        data_min = np.min(landsat_data)\n",
        "        data_max = np.max(landsat_data)\n",
        "        print(f\"Landsat Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "        if data_max > 0:\n",
        "            landsat_data_normalized = landsat_data / data_max\n",
        "            print(\"Landsat Data normalized to [0, 1].\")\n",
        "            prepared_hsi_data = {\n",
        "                'data': landsat_data_normalized,\n",
        "                'gt': landsat_gt,\n",
        "                'dataset': 'Landsat'\n",
        "            }\n",
        "            print(\"Prepared data for Landsat stored in 'prepared_hsi_data'.\")\n",
        "        else:\n",
        "            print(\"Landsat Data max is 0, skipping normalization.\")\n",
        "            prepared_hsi_data = {\n",
        "                'data': landsat_data,\n",
        "                'gt': landsat_gt,\n",
        "                'dataset': 'Landsat'\n",
        "            }\n",
        "            print(\"Prepared data for Landsat stored in 'prepared_hsi_data' without normalization.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing Landsat data: {e}\")\n",
        "        prepared_hsi_data = None\n",
        "\n",
        "else:\n",
        "    print(\"Could not identify complete .npy files for either PaviaU or Landsat datasets based on specific names.\")\n",
        "    prepared_hsi_data = None\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified PaviaU data at: AGTC/AGTC-HSI/PaviaU/Data-Preparation/PaviaU.npy\n",
            "Identified PaviaU ground truth at: AGTC/AGTC-HSI/PaviaU/Metrics/PaviaU_ground_truth.npy\n",
            "PaviaU data loaded successfully.\n",
            "PaviaU Data shape: (610, 340, 103)\n",
            "PaviaU Ground Truth shape: (610, 340, 103)\n",
            "PaviaU Data original range: [0, 8000]\n",
            "PaviaU Data normalized to [0, 1].\n",
            "Prepared data for PaviaU stored in 'prepared_hsi_data'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39a03a8e"
      },
      "source": [
        "## Train the agtc-hsi model\n",
        "\n",
        "### Subtask:\n",
        "Execute the code for training the AGTC-HSI model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66813d6f"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the main training script for the AGTC-HSI model, likely `main.py` or `train.py`, passing necessary arguments like the dataset name.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae1bbac4",
        "outputId": "7347a363-9a88-4fcb-b7bd-df3360157f72"
      },
      "source": [
        "import os\n",
        "\n",
        "# Navigate to the AGTC-HSI directory\n",
        "hsi_dir = os.path.join('AGTC', 'AGTC-HSI')\n",
        "print(f\"Navigating to {hsi_dir}\")\n",
        "os.chdir(hsi_dir)\n",
        "\n",
        "# Inspect files to find the training script (simulated inspection)\n",
        "# Based on typical project structures, main.py or train.py are common.\n",
        "# Let's assume the training script is main.py in the AGTC-HSI directory.\n",
        "\n",
        "train_script = 'main.py'\n",
        "\n",
        "# Determine which dataset is available in prepared_hsi_data\n",
        "if 'prepared_hsi_data' in globals() and prepared_hsi_data is not None and 'dataset' in prepared_hsi_data:\n",
        "    dataset_name = prepared_hsi_data['dataset']\n",
        "    print(f\"Prepared data for dataset: {dataset_name}\")\n",
        "\n",
        "    # Construct the command to run the training script\n",
        "    # The script likely expects the dataset name as an argument.\n",
        "    # Based on typical command-line interfaces for deep learning training scripts,\n",
        "    # a common format is `python script_name --dataset dataset_name`.\n",
        "    # We'll assume this format or a similar one is used.\n",
        "    # If the script requires other arguments, they would need to be added here.\n",
        "    # For simplicity, we'll start with just the dataset name.\n",
        "\n",
        "    command = f\"python {train_script} --dataset {dataset_name}\"\n",
        "    print(f\"Executing training command: {command}\")\n",
        "\n",
        "    # Execute the command\n",
        "    !{command}\n",
        "\n",
        "else:\n",
        "    print(\"Prepared HSI data not found or dataset name is missing. Cannot proceed with training.\")\n",
        "\n",
        "\n",
        "# Navigate back to the original directory\n",
        "os.chdir('..')\n",
        "os.chdir('..')\n",
        "print(\"Navigated back to the original directory.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to AGTC/AGTC-HSI\n",
            "Prepared data for dataset: PaviaU\n",
            "Executing training command: python main.py --dataset PaviaU\n",
            "python3: can't open file '/content/AGTC/AGTC-HSI/main.py': [Errno 2] No such file or directory\n",
            "Navigated back to the original directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "478ec550"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to run `main.py` failed because the file was not found in the `AGTC-HSI` directory. I need to re-inspect the files within the `AGTC-HSI` directory to find the correct training script and then execute it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f2047a0",
        "outputId": "aa684019-27b1-4def-8058-5ec3e4046e9c"
      },
      "source": [
        "import os\n",
        "\n",
        "# Navigate to the AGTC-HSI directory\n",
        "hsi_dir = os.path.join('AGTC', 'AGTC-HSI')\n",
        "print(f\"Navigating to {hsi_dir}\")\n",
        "os.chdir(hsi_dir)\n",
        "\n",
        "# List files in the current directory to identify the training script\n",
        "print(f\"Listing files in {os.getcwd()}:\")\n",
        "!ls -l\n",
        "\n",
        "# Based on the output of the file listing, identify the correct training script.\n",
        "# It might be named differently, e.g., 'train.py', or be located in a subdirectory.\n",
        "# Assuming the output reveals a script like 'train.py' or similar.\n",
        "# Let's assume 'train.py' exists based on common practices.\n",
        "# If 'train.py' is not found, further inspection of the file listing will be needed.\n",
        "\n",
        "train_script = 'train.py' # Assuming 'train.py' based on convention\n",
        "\n",
        "# Determine which dataset is available in prepared_hsi_data\n",
        "if 'prepared_hsi_data' in globals() and prepared_hsi_data is not None and 'dataset' in prepared_hsi_data:\n",
        "    dataset_name = prepared_hsi_data['dataset']\n",
        "    print(f\"Prepared data for dataset: {dataset_name}\")\n",
        "\n",
        "    # Construct the command to run the training script\n",
        "    # Adjust the command based on the identified script name.\n",
        "    command = f\"python {train_script} --dataset {dataset_name}\"\n",
        "    print(f\"Executing training command: {command}\")\n",
        "\n",
        "    # Execute the command\n",
        "    !{command}\n",
        "\n",
        "else:\n",
        "    print(\"Prepared HSI data not found or dataset name is missing. Cannot proceed with training.\")\n",
        "\n",
        "# Navigate back to the original directory\n",
        "os.chdir('..')\n",
        "os.chdir('..')\n",
        "print(\"Navigated back to the original directory.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to AGTC/AGTC-HSI\n",
            "Listing files in /content/AGTC/AGTC-HSI:\n",
            "total 8\n",
            "drwxr-xr-x 5 root root 4096 Oct 21 15:02 Landsat\n",
            "drwxr-xr-x 5 root root 4096 Oct 21 15:02 PaviaU\n",
            "Prepared data for dataset: PaviaU\n",
            "Executing training command: python train.py --dataset PaviaU\n",
            "python3: can't open file '/content/AGTC/AGTC-HSI/train.py': [Errno 2] No such file or directory\n",
            "Navigated back to the original directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a1b4f2"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ls -l` output shows that there are only subdirectories named 'Landsat' and 'PaviaU' within `AGTC/AGTC-HSI`, and no `main.py` or `train.py` script directly in this directory. This suggests the training script is likely located within one of these dataset-specific subdirectories or in a different parent directory. Given that the goal is to train the AGTC-HSI model using the prepared PaviaU data, I will navigate into the 'PaviaU' directory and list its contents to find the training script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1735c6e5",
        "outputId": "1603b583-f953-4a91-cc04-6804d818fe3c"
      },
      "source": [
        "import os\n",
        "\n",
        "# Navigate to the AGTC-HSI directory\n",
        "hsi_dir = os.path.join('AGTC', 'AGTC-HSI')\n",
        "print(f\"Navigating to {hsi_dir}\")\n",
        "os.chdir(hsi_dir)\n",
        "\n",
        "# Navigate into the PaviaU directory\n",
        "paviau_dir = 'PaviaU'\n",
        "print(f\"Navigating into {paviau_dir}\")\n",
        "os.chdir(paviau_dir)\n",
        "\n",
        "# List files in the current directory to identify the training script\n",
        "print(f\"Listing files in {os.getcwd()}:\")\n",
        "!ls -l\n",
        "\n",
        "# Based on the output of the file listing, identify the correct training script.\n",
        "# It might be named 'main.py', 'train.py', or something else.\n",
        "# Once identified, attempt to execute it.\n",
        "# For now, I will assume the script is named `main.py` or `train.py` and prepare for execution.\n",
        "# The actual execution command will depend on the listed files.\n",
        "# If the listing reveals a script, I will use that name.\n",
        "\n",
        "# Since I cannot predict the exact script name, I will stop here and wait for the output\n",
        "# to inform the next step of constructing the execution command.\n",
        "\n",
        "# Do NOT navigate back yet, stay in the PaviaU directory to prepare for execution.\n",
        "# The next step will involve constructing and running the command based on the `ls -l` output."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to AGTC/AGTC-HSI\n",
            "Navigating into PaviaU\n",
            "Listing files in /content/AGTC/AGTC-HSI/PaviaU:\n",
            "total 89100\n",
            "drwxr-xr-x 2 root root     4096 Oct 21 15:02 Data-Preparation\n",
            "-rw-r--r-- 1 root root     6783 Oct 21 15:01 main_net.py\n",
            "drwxr-xr-x 3 root root     4096 Oct 21 15:02 Metrics\n",
            "-rw-r--r-- 1 root root 37190141 Oct 21 15:01 PaviaU_test.mat\n",
            "-rw-r--r-- 1 root root 54001792 Oct 21 15:02 PaviaU_test.npy\n",
            "-rw-r--r-- 1 root root     1896 Oct 21 15:01 README.md\n",
            "-rw-r--r-- 1 root root     2009 Oct 21 15:01 test.py\n",
            "-rw-r--r-- 1 root root     3812 Oct 21 15:01 train.py\n",
            "-rw-r--r-- 1 root root     6120 Oct 21 15:01 util.py\n",
            "drwxr-xr-x 2 root root     4096 Oct 21 15:01 Weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ee75875"
      },
      "source": [
        "**Reasoning**:\n",
        "The output of the `ls -l` command in the PaviaU directory clearly shows a file named `train.py`. This is likely the training script for the AGTC-HSI model on the PaviaU dataset. I will now execute this script. Since the dataset is already prepared and the script is within the PaviaU directory, the script might not require the dataset name as a command-line argument, or it might expect it in a different format. I will attempt to run `python train.py` first. If it requires arguments, the error message will provide clues for the next step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# We are already in the AGTC/AGTC-HSI/PaviaU directory from the previous step.\n",
        "\n",
        "train_script = 'train.py'\n",
        "print(f\"Executing training script: {train_script}\")\n",
        "\n",
        "# Execute the training script\n",
        "# Assuming train.py can be run directly without command-line arguments\n",
        "# if the data is in the same directory or accessible globally (like 'prepared_hsi_data').\n",
        "# If it requires arguments, the execution will fail and we will inspect the script or error message.\n",
        "!python {train_script}\n",
        "\n",
        "# After execution, navigate back to the original directory\n",
        "os.chdir('..') # Go back to AGTC/AGTC-HSI\n",
        "os.chdir('..') # Go back to AGTC\n",
        "print(\"Navigated back to the original directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN5JMHJVUtwN",
        "outputId": "0a87392d-72ba-44ce-e86a-cd4384dbc3a8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing training script: train.py\n",
            "python3: can't open file '/content/train.py': [Errno 2] No such file or directory\n",
            "Navigated back to the original directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21d932a5"
      },
      "source": [
        "## Download Project and Notebook\n",
        "\n",
        "### Subtask:\n",
        "Create a zip file containing the cloned AGTC repository and the current notebook, and provide code to download the zip file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc49d7ca"
      },
      "source": [
        "## Install missing dependency: torchinfo\n",
        "\n",
        "### Subtask:\n",
        "Install the `torchinfo` library required for the Landsat training script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7235c117"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution of `train.py` for Landsat failed due to a `ModuleNotFoundError` for `torchinfo`. Install this library using pip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2c35ebc",
        "outputId": "c3ad6b1a-f4d2-493b-d748-c0b356857d7c"
      },
      "source": [
        "print(\"Installing torchinfo...\")\n",
        "!pip install torchinfo\n",
        "print(\"torchinfo installed.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing torchinfo...\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "torchinfo installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67a3c6d0"
      },
      "source": [
        "import os\n",
        "\n",
        "# Ensure we are in the /content directory first\n",
        "content_dir = '/content/'\n",
        "print(f\"Navigating to {content_dir}\")\n",
        "os.chdir(content_dir)\n",
        "\n",
        "# Navigate to the AGTC-HSI Landsat directory\n",
        "hsi_landsat_dir = os.path.join('AGTC', 'AGTC-HSI', 'Landsat')\n",
        "print(f\"Navigating to {hsi_landsat_dir}\")\n",
        "os.chdir(hsi_landsat_dir)\n",
        "\n",
        "# List files in the current directory to identify the training script\n",
        "print(f\"Listing files in {os.getcwd()}:\")\n",
        "!ls -l\n",
        "\n",
        "# Based on the file listing, identify the correct training script.\n",
        "# Assuming it's named 'train.py' similar to the PaviaU directory.\n",
        "train_script = 'train.py' # Assuming 'train.py'\n",
        "\n",
        "# Check if the training script exists before attempting to run it\n",
        "if os.path.exists(train_script):\n",
        "    print(f\"Executing training script: {train_script}\")\n",
        "\n",
        "    # Execute the training script with the --data_path argument\n",
        "    # We need the path to the prepared Landsat data .npy file.\n",
        "    # Based on the previous data preparation step, this path is likely in prepared_hsi_data_landsat.\n",
        "    # Let's assume the structure of prepared_hsi_data_landsat includes the path or we can reconstruct it.\n",
        "    # A more robust way is to get the path from the variable if it's available.\n",
        "    # If prepared_hsi_data_landsat is not available or doesn't contain the path,\n",
        "    # we would need to re-identify the data file path.\n",
        "\n",
        "    # Re-identifying the landsat data path for safety\n",
        "    agtc_root_dir = os.path.join('/content/', 'AGTC')\n",
        "    landsat_data_path = None\n",
        "    # Re-list .npy files to find the landsat training data path\n",
        "    npy_files_found = []\n",
        "    for root, dirs, files in os.walk(agtc_root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.npy'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                npy_files_found.append(file_path)\n",
        "\n",
        "    for file_path in npy_files_found:\n",
        "        lower_file_path = file_path.lower()\n",
        "        if 'landsat' in lower_file_path and 'training_clean.npy' in lower_file_path:\n",
        "            landsat_data_path = file_path\n",
        "            break # Found the specific training data file\n",
        "\n",
        "    if landsat_data_path:\n",
        "        print(f\"Identified Landsat training data path: {landsat_data_path}\")\n",
        "        command = f\"python {train_script} --data_path {landsat_data_path}\"\n",
        "        print(f\"Executing training command: {command}\")\n",
        "        !{command}\n",
        "    else:\n",
        "        print(\"Could not identify the Landsat training data .npy file path.\")\n",
        "        print(\"Cannot execute training script without --data_path.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"Training script '{train_script}' not found in {os.getcwd()}.\")\n",
        "    print(\"Please check the directory contents and update the script name if necessary.\")\n",
        "\n",
        "\n",
        "# After execution (or if script not found), navigate back to the original directory\n",
        "# We need to go back two levels from the current Landsat directory to reach /content\n",
        "os.chdir('..') # Go back to AGTC/AGTC-HSI\n",
        "os.chdir('..') # Go back to AGTC\n",
        "os.chdir('..') # Go back to /content\n",
        "print(\"Navigated back to the original directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63fb3003"
      },
      "source": [
        "## Download Output Files (Landsat)\n",
        "\n",
        "### Subtask:\n",
        "Provide code to download the output files generated by the AGTC-HSI model training for the Landsat dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95084f3b"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the output files generated during Landsat training (e.g., trained model weights, evaluation results) within the Landsat directory and provide code to download them using `google.colab.files.download`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "83dac841",
        "outputId": "1c834705-3733-4af5-8966-e6d02f280c76"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the directory where output files for Landsat are expected\n",
        "# Based on the repository structure and typical training outputs,\n",
        "# model weights and results might be saved within the Landsat directory\n",
        "# or a subdirectory within it (e.g., 'Weight', 'Metrics', 'results').\n",
        "# We will check common locations within the Landsat directory.\n",
        "\n",
        "landsat_output_dirs_to_check = [\n",
        "    os.path.join('AGTC', 'AGTC-HSI', 'Landsat', 'Weight'),\n",
        "    os.path.join('AGTC', 'AGTC-HSI', 'Landsat', 'Metrics'),\n",
        "    os.path.join('AGTC', 'AGTC-HSI', 'Landsat', 'results') # Common name for results directory\n",
        "]\n",
        "\n",
        "found_output_files = False\n",
        "\n",
        "for output_dir in landsat_output_dirs_to_check:\n",
        "    print(f\"Checking for output files in: {output_dir}\")\n",
        "    if os.path.exists(output_dir):\n",
        "        print(f\"Listing files in {output_dir}:\")\n",
        "        # List files, excluding directories and potentially large data files\n",
        "        output_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]\n",
        "\n",
        "        if output_files:\n",
        "            found_output_files = True\n",
        "            print(\"Found output files:\")\n",
        "            for file_path in output_files:\n",
        "                print(f\"- {file_path}\")\n",
        "                try:\n",
        "                    print(f\"Downloading {os.path.basename(file_path)}...\")\n",
        "                    files.download(file_path)\n",
        "                    print(\"Download complete.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading {file_path}: {e}\")\n",
        "        else:\n",
        "            print(\"No output files found in this directory.\")\n",
        "    else:\n",
        "        print(\"Output directory not found.\")\n",
        "\n",
        "if not found_output_files:\n",
        "    print(\"\\nNo output files were found in the checked directories for the Landsat training.\")\n",
        "    print(\"You may need to inspect the Landsat training script (train.py) to determine where output files are saved.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for output files in: AGTC/AGTC-HSI/Landsat/Weight\n",
            "Listing files in AGTC/AGTC-HSI/Landsat/Weight:\n",
            "Found output files:\n",
            "- AGTC/AGTC-HSI/Landsat/Weight/AGTC-Landsat.pth\n",
            "Downloading AGTC-Landsat.pth...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75e78e2d-e400-41f6-ae72-5739830fbdb0\", \"AGTC-Landsat.pth\", 28603765)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "Checking for output files in: AGTC/AGTC-HSI/Landsat/Metrics\n",
            "Listing files in AGTC/AGTC-HSI/Landsat/Metrics:\n",
            "Found output files:\n",
            "- AGTC/AGTC-HSI/Landsat/Metrics/Landsat_ground_truth.mat\n",
            "Downloading Landsat_ground_truth.mat...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f74ebd30-d10d-46cb-a26d-ff47d7404515\", \"Landsat_ground_truth.mat\", 1028526)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "- AGTC/AGTC-HSI/Landsat/Metrics/metrics.m\n",
            "Downloading metrics.m...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9970141c-74dd-4742-b6da-09f82c371c00\", \"metrics.m\", 717)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "- AGTC/AGTC-HSI/Landsat/Metrics/Landsat_ground_truth.npy\n",
            "Downloading Landsat_ground_truth.npy...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_53365049-4033-4b63-bb13-70e0a823c8af\", \"Landsat_ground_truth.npy\", 2097280)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "Checking for output files in: AGTC/AGTC-HSI/Landsat/results\n",
            "Output directory not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c767418",
        "outputId": "edb2c68b-8a02-445f-cd7c-c00c79390dd8"
      },
      "source": [
        "import os\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(\"Listing contents of the current directory:\")\n",
        "!ls -l"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /\n",
            "Listing contents of the current directory:\n",
            "total 448\n",
            "lrwxrwxrwx   1 root root      7 Jun 27  2024 bin -> usr/bin\n",
            "drwxr-xr-x   2 root root   4096 Apr 18  2022 boot\n",
            "drwxr-xr-x   1 root root   4096 Oct 21 15:01 content\n",
            "-rw-r--r--   1 root root   4332 Jul 10  2024 cuda-keyring_1.1-1_all.deb\n",
            "drwxr-xr-x   1 root root   4096 Oct 17 22:44 datalab\n",
            "drwxr-xr-x   6 root root    460 Oct 21 14:57 dev\n",
            "drwxr-xr-x   1 root root   4096 Oct 21 14:57 etc\n",
            "drwxr-xr-x   2 root root   4096 Apr 18  2022 home\n",
            "drwxr-xr-x   3 root root   4096 Oct 21 14:57 kaggle\n",
            "lrwxrwxrwx   1 root root      7 Jun 27  2024 lib -> usr/lib\n",
            "lrwxrwxrwx   1 root root      9 Jun 27  2024 lib32 -> usr/lib32\n",
            "lrwxrwxrwx   1 root root      9 Jun 27  2024 lib64 -> usr/lib64\n",
            "lrwxrwxrwx   1 root root     10 Jun 27  2024 libx32 -> usr/libx32\n",
            "drwxr-xr-x   2 root root   4096 Jun 27  2024 media\n",
            "drwxr-xr-x   2 root root   4096 Jun 27  2024 mnt\n",
            "-rw-r--r--   1 root root  17294 Jul 10  2024 NGC-DL-CONTAINER-LICENSE\n",
            "drwxr-xr-x   1 root root   4096 Oct 21 14:57 opt\n",
            "dr-xr-xr-x 206 root root      0 Oct 21 14:57 proc\n",
            "drwxrwxr-x  14 root root   4096 Oct 17 22:11 python-apt\n",
            "-r-xr-xr-x   1 root root 346012 Jan  1  2000 python-apt.tar.xz\n",
            "drwx------   1 root root   4096 Oct 21 14:57 root\n",
            "drwxr-xr-x   1 root root   4096 Oct 17 22:04 run\n",
            "lrwxrwxrwx   1 root root      8 Jun 27  2024 sbin -> usr/sbin\n",
            "drwxr-xr-x   2 root root   4096 Jun 27  2024 srv\n",
            "dr-xr-xr-x  13 root root      0 Oct 21 14:57 sys\n",
            "drwxrwxrwt   1 root root   4096 Oct 21 15:13 tmp\n",
            "drwxr-xr-x   1 root root   4096 Oct 17 22:44 tools\n",
            "drwxr-xr-x   1 root root   4096 Oct 21 14:57 usr\n",
            "drwxr-xr-x   1 root root   4096 Oct 17 22:45 var\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8121d7e"
      },
      "source": [
        "## Prepare data for AGTC-HSI (Landsat)\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the Landsat data (including the converted `.npy` files) specifically for the AGTC-HSI model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1f66fcf"
      },
      "source": [
        "**Reasoning**:\n",
        "Load and preprocess the Landsat data and ground truth .npy files based on the previously identified paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddbd7648",
        "outputId": "5e92d4ce-682c-443b-99fc-e4d5d3702aa2"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Identify the correct data and ground truth files for Landsat based on observed names and shapes\n",
        "agtc_root_dir = 'AGTC'\n",
        "landsat_data_path = None # Prioritizing clean training data\n",
        "landsat_gt_path = None\n",
        "\n",
        "# Re-list .npy files to ensure we have the latest paths if needed, though should be the same as before\n",
        "npy_files_found = []\n",
        "for root, dirs, files in os.walk(agtc_root_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.npy'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            npy_files_found.append(file_path)\n",
        "\n",
        "for file_path in npy_files_found:\n",
        "    lower_file_path = file_path.lower()\n",
        "    if 'landsat' in lower_file_path:\n",
        "         if 'ground_truth' in lower_file_path:\n",
        "            landsat_gt_path = file_path\n",
        "         elif 'landsat7_training_clean.npy' in lower_file_path: # Look for the clean training data\n",
        "            landsat_data_path = file_path\n",
        "\n",
        "\n",
        "# Now attempt to load and preprocess using the identified Landsat paths\n",
        "prepared_hsi_data_landsat = None # Use a new variable for Landsat data\n",
        "\n",
        "if landsat_data_path and landsat_gt_path:\n",
        "    print(f\"Identified Landsat data at: {landsat_data_path}\")\n",
        "    print(f\"Identified Landsat ground truth at: {landsat_gt_path}\")\n",
        "\n",
        "    try:\n",
        "        landsat_data = np.load(landsat_data_path)\n",
        "        landsat_gt = np.load(landsat_gt_path)\n",
        "\n",
        "        print(\"Landsat data loaded successfully.\")\n",
        "        print(\"Landsat Data shape:\", landsat_data.shape)\n",
        "        print(\"Landsat Ground Truth shape:\", landsat_gt.shape)\n",
        "\n",
        "        # Basic preprocessing: Normalization\n",
        "        data_min = np.min(landsat_data)\n",
        "        data_max = np.max(landsat_data)\n",
        "        print(f\"Landsat Data original range: [{data_min}, {data_max}]\")\n",
        "\n",
        "        if data_max > 0:\n",
        "            landsat_data_normalized = landsat_data / data_max\n",
        "            print(\"Landsat Data normalized to [0, 1].\")\n",
        "            prepared_hsi_data_landsat = {\n",
        "                'data': landsat_data_normalized,\n",
        "                'gt': landsat_gt,\n",
        "                'dataset': 'Landsat'\n",
        "            }\n",
        "            print(\"Prepared data for Landsat stored in 'prepared_hsi_data_landsat'.\")\n",
        "        else:\n",
        "            print(\"Landsat Data max is 0, skipping normalization.\")\n",
        "            prepared_hsi_data_landsat = {\n",
        "                'data': landsat_data,\n",
        "                'gt': landsat_gt,\n",
        "                'dataset': 'Landsat'\n",
        "            }\n",
        "            print(\"Prepared data for Landsat stored in 'prepared_hsi_data_landsat' without normalization.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing Landsat data: {e}\")\n",
        "        prepared_hsi_data_landsat = None\n",
        "\n",
        "else:\n",
        "    print(\"Could not identify complete .npy files for the Landsat dataset based on specific names.\")\n",
        "    prepared_hsi_data_landsat = None # Indicate failure to load data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not identify complete .npy files for the Landsat dataset based on specific names.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "219ddf1c"
      },
      "source": [
        "## Train the AGTC-HSI model (Landsat)\n",
        "\n",
        "### Subtask:\n",
        "Execute the code for training the AGTC-HSI model using the Landsat dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3396ba"
      },
      "source": [
        "**Reasoning**:\n",
        "Navigate to the AGTC-HSI Landsat directory, identify the training script, and execute it with the Landsat data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b19a708",
        "outputId": "3331e791-de58-425e-af48-4bf94285be29"
      },
      "source": [
        "import os\n",
        "\n",
        "# Ensure we are in the /content directory\n",
        "content_dir = '/content/'\n",
        "print(f\"Navigating to {content_dir}\")\n",
        "os.chdir(content_dir)\n",
        "\n",
        "# Navigate to the AGTC-HSI directory to list its contents\n",
        "hsi_dir = os.path.join('AGTC', 'AGTC-HSI')\n",
        "print(f\"Navigating to {hsi_dir}\")\n",
        "\n",
        "# Navigate to the AGTC/AGTC-HSI directory\n",
        "os.chdir(hsi_dir)\n",
        "\n",
        "\n",
        "# List files and directories in the current directory to identify the Landsat directory\n",
        "print(f\"Listing files in {os.getcwd()}:\")\n",
        "!ls -l\n",
        "\n",
        "# Based on the file listing, identify the correct path to the Landsat directory.\n",
        "# Assuming the output reveals a directory named 'Landsat' or similar.\n",
        "# Once identified, navigate into that directory and list its contents to find the training script.\n",
        "\n",
        "# For now, I will stop here after listing the contents and wait for the output\n",
        "# to inform the next step of navigating to the correct Landsat directory and finding the script.\n",
        "\n",
        "# Do NOT navigate back yet, stay in the AGTC/AGTC-HSI directory to prepare for further navigation.\n",
        "# The next step will involve navigating to the correct Landsat directory based on the `ls -l` output."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to /content/\n",
            "Navigating to AGTC/AGTC-HSI\n",
            "Listing files in /content/AGTC/AGTC-HSI:\n",
            "total 8\n",
            "drwxr-xr-x 5 root root 4096 Oct 21 15:02 Landsat\n",
            "drwxr-xr-x 6 root root 4096 Oct 21 15:06 PaviaU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a99a527"
      },
      "source": [
        "**Reasoning**:\n",
        "Use shell commands to create a zip archive of the AGTC directory and the current notebook file, then use `google.colab.files.download` to download the created zip file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "d0c1f0b8",
        "outputId": "ec55d47c-a758-4de6-fa6c-cac654be46e7"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "# from google.colab import notebook # notebook module is not available in this environment\n",
        "\n",
        "# Define the name for the zip file\n",
        "zip_filename = \"AGTC_directory.zip\" # Changed filename to reflect only the directory\n",
        "\n",
        "# Get the path to the directory to be zipped\n",
        "agtc_dir_path = '/content/AGTC'\n",
        "\n",
        "# Ensure the AGTC directory exists before zipping\n",
        "if os.path.exists(agtc_dir_path):\n",
        "    print(f\"Creating zip archive '{zip_filename}' containing '{agtc_dir_path}'...\")\n",
        "    # Use the zip command to create the archive\n",
        "    # The -r flag includes directories recursively\n",
        "    # The -q flag makes the output quiet\n",
        "    # The -x *.ipynb_checkpoints* excludes notebook checkpoints\n",
        "    # Zipping only the AGTC directory\n",
        "    !zip -r -q {zip_filename} {agtc_dir_path} -x \"*.ipynb_checkpoints*\"\n",
        "\n",
        "    print(f\"Zip archive '{zip_filename}' created.\")\n",
        "\n",
        "    # Provide the download link for the zip file\n",
        "    print(f\"Downloading '{zip_filename}'...\")\n",
        "    try:\n",
        "        files.download(zip_filename)\n",
        "        print(\"Download complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {zip_filename}: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Directory '{agtc_dir_path}' not found.\")\n",
        "    print(\"Please ensure the AGTC repository was cloned successfully.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating zip archive 'AGTC_directory.zip' containing '/content/AGTC'...\n",
            "Zip archive 'AGTC_directory.zip' created.\n",
            "Downloading 'AGTC_directory.zip'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_653d2a0a-a0a7-42d8-8490-23653937efee\", \"AGTC_directory.zip\", 513711146)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fb9200c"
      },
      "source": [
        "## Download Output Files\n",
        "\n",
        "### Subtask:\n",
        "Provide code to download the output files generated by the AGTC-HSI model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacf68ee"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the output files generated during training (e.g., trained model weights, evaluation results) and provide code to download them using `google.colab.files.download`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e9768bf",
        "outputId": "9d892ceb-7fdb-4e07-a0ee-dacbdf08150c"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Navigate to the directory where output files are expected\n",
        "# Based on the repository structure and typical training outputs,\n",
        "# model weights are often saved in a 'Weight' or 'models' directory,\n",
        "# and results might be in a 'Metrics' or 'results' directory.\n",
        "# From the previous `ls -l` output in the PaviaU directory, there is a 'Weight' directory.\n",
        "\n",
        "output_dir = os.path.join('AGTC', 'AGTC-HSI', 'PaviaU', 'Weight') # Assuming weights are saved here\n",
        "print(f\"Checking for output files in: {output_dir}\")\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    print(f\"Listing files in {output_dir}:\")\n",
        "    output_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]\n",
        "\n",
        "    if output_files:\n",
        "        print(\"Found output files:\")\n",
        "        for file_path in output_files:\n",
        "            print(f\"- {file_path}\")\n",
        "            try:\n",
        "                print(f\"Downloading {os.path.basename(file_path)}...\")\n",
        "                files.download(file_path)\n",
        "                print(\"Download complete.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {file_path}: {e}\")\n",
        "    else:\n",
        "        print(\"No output files found in the expected directory.\")\n",
        "else:\n",
        "    print(\"Output directory not found.\")\n",
        "\n",
        "# Also check for potential output files in the Metrics directory\n",
        "metrics_dir = os.path.join('AGTC', 'AGTC-HSI', 'PaviaU', 'Metrics')\n",
        "print(f\"\\nChecking for output files in: {metrics_dir}\")\n",
        "\n",
        "if os.path.exists(metrics_dir):\n",
        "    print(f\"Listing files in {metrics_dir}:\")\n",
        "    metrics_files = [os.path.join(metrics_dir, f) for f in os.listdir(metrics_dir) if os.path.isfile(os.path.join(metrics_dir, f)) and not f.endswith('.npy')] # Exclude the ground truth .npy file\n",
        "\n",
        "    if metrics_files:\n",
        "        print(\"Found output files in Metrics directory:\")\n",
        "        for file_path in metrics_files:\n",
        "            print(f\"- {file_path}\")\n",
        "            try:\n",
        "                print(f\"Downloading {os.path.basename(file_path)}...\")\n",
        "                files.download(file_path)\n",
        "                print(\"Download complete.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {file_path}: {e}\")\n",
        "    else:\n",
        "        print(\"No additional output files found in the Metrics directory.\")\n",
        "else:\n",
        "    print(\"Metrics directory not found.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for output files in: AGTC/AGTC-HSI/PaviaU/Weight\n",
            "Output directory not found.\n",
            "\n",
            "Checking for output files in: AGTC/AGTC-HSI/PaviaU/Metrics\n",
            "Metrics directory not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oDKMjQMCYYga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}